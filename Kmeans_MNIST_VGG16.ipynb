{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b355014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98673359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e333d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "# Modify first layer with 1 channel\n",
    "vgg16.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "# Remove the last fully connected layer to get feature extraction layer\n",
    "vgg16.features = torch.nn.Sequential(*list(vgg16.features.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacfceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16 = vgg16.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e256a28",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f09ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each image in the dataset\n",
    "features = []\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in train_dataset:\n",
    "        #print(i)\n",
    "\n",
    "        image = image.to(device)\n",
    "        features.append(vgg16(image.unsqueeze(0)).cpu().numpy().flatten())\n",
    "        #labels.append(labels)\n",
    "        #i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9747c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes forever\n",
    "# # Convert the features and labels to numpy arrays\n",
    "# features = np.array(features)\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# # Save the features and labels to a compressed npz file\n",
    "# np.savez_compressed('mnist_features_labels.npz', features=features, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2e91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the compressed npz file\n",
    "# data = np.load('mnist_features_labels.npz')\n",
    "\n",
    "# # Extract the features and labels from the data dictionary\n",
    "# features = data['features']\n",
    "# labels = data['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a949350",
   "metadata": {},
   "source": [
    "# Kmeans and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0b2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering with 10 clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99873395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0, 4, 3, 2, 8, 0, 7, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "cluster_labels = []\n",
    "for i in range(10):\n",
    "    cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "    cluster_mnist_labels = train_dataset.targets[cluster_indices]\n",
    "    cluster_labels.append(np.bincount(cluster_mnist_labels).argmax())\n",
    "#some numbers are not showing\n",
    "#prob becuz some numbers have similar features with multiple classes\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c65773da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dataset = [[] for i in range(10)]\n",
    "\n",
    "for i in range(len(features)):\n",
    "    cluster_label = kmeans.labels_[i] #return the cluster label assigned to the ith dataset\n",
    "    clustered_dataset[cluster_label].append((train_dataset[i][0], train_dataset[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1265abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster0:  4328 datasets\n",
      "Cluster1:  8039 datasets\n",
      "Cluster2:  6204 datasets\n",
      "Cluster3:  3747 datasets\n",
      "Cluster4:  4631 datasets\n",
      "Cluster5:  8202 datasets\n",
      "Cluster6:  6351 datasets\n",
      "Cluster7:  4156 datasets\n",
      "Cluster8:  7459 datasets\n",
      "Cluster9:  6883 datasets\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Cluster{i}: \" ,len(clustered_dataset[i]), \"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30341725",
   "metadata": {},
   "source": [
    "# Testing the Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "482c38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for creating custom dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, data_dict, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.keys = list(data_dict.keys())\n",
    "        self.keys.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image data and label for the current item\n",
    "        key = self.keys[index]\n",
    "        img = self.data_dict[key]['images']\n",
    "        label = self.data_dict[key]['labels']\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af429c",
   "metadata": {},
   "source": [
    "Training with the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b88a21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # Flatten the input image\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load the train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the dataloader for the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e32f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███                           | 474/4690 [00:09<01:27, 47.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [469/469], Loss: 2.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████                        | 944/4690 [00:18<01:15, 49.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [469/469], Loss: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|████████▋                    | 1413/4690 [00:28<01:04, 50.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [469/469], Loss: 0.5875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███████████▋                 | 1881/4690 [00:37<00:57, 48.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [469/469], Loss: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|██████████████▌              | 2353/4690 [00:47<00:53, 43.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [469/469], Loss: 0.3540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████████████▍           | 2819/4690 [00:57<00:36, 50.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [469/469], Loss: 0.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████▎        | 3289/4690 [01:06<00:30, 46.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [469/469], Loss: 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████████████████████▏     | 3756/4690 [01:16<00:19, 46.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [469/469], Loss: 0.2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|██████████████████████████▏  | 4228/4690 [01:25<00:09, 49.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [469/469], Loss: 0.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4690/4690 [01:34<00:00, 49.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [469/469], Loss: 0.1423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model with tqdm progress bar\n",
    "total_iterations = len(train_loader) * num_epochs\n",
    "with tqdm(total=total_iterations, desc='Training') as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i == len(train_loader)-1):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6b08a",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5fe7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset with 50% of data\n",
    "data_dict_50 = {}\n",
    "index = 0\n",
    "for i in range(10):\n",
    "    num_items = len(clustered_dataset[i])\n",
    "    num_selected = len(clustered_dataset[i])/2\n",
    "    #select random indices\n",
    "    selected_indices = random.sample(range(num_items), int(num_selected))\n",
    "    for j in selected_indices:\n",
    "        data_dict_50[index] = {'images': clustered_dataset[i][j][0], 'labels': clustered_dataset[i][j][1]}\n",
    "        index = index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b62701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dict_50)\n",
    "\n",
    "custom_dataset = DictDataset(data_dict_50)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model1 = MLP()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "\n",
    "# Create the dataloader for the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5489434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|███▏                         | 259/2350 [00:02<00:16, 123.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [235/235], Loss: 1.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██████                       | 496/2350 [00:03<00:13, 136.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [235/235], Loss: 0.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|████████▉                    | 728/2350 [00:05<00:13, 122.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [235/235], Loss: 0.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|███████████▉                 | 966/2350 [00:07<00:10, 129.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [235/235], Loss: 0.2316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|██████████████▏             | 1189/2350 [00:09<00:09, 128.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [235/235], Loss: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|████████████████▉           | 1426/2350 [00:11<00:08, 115.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [235/235], Loss: 0.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████████████████▉        | 1669/2350 [00:13<00:05, 126.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [235/235], Loss: 0.1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|██████████████████████▌     | 1894/2350 [00:14<00:03, 124.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [235/235], Loss: 0.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████████████████████▍  | 2139/2350 [00:16<00:01, 131.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [235/235], Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 2350/2350 [00:18<00:00, 126.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [235/235], Loss: 0.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_iterations = len(train_loader) * num_epochs\n",
    "with tqdm(total=total_iterations, desc='Training') as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model1(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i == len(train_loader)-1):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7259d5a",
   "metadata": {},
   "source": [
    "# testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "392201bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy training with the whole dataset: 97.60%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test accuracy training with the whole dataset: {100 * correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dac7c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 50% dataset is: 95.15%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test accuracy for 50% dataset is: {100 * correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947995e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
